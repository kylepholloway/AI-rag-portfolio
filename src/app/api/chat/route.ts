import { openai } from '@/utils/openai'
import { drizzle } from 'drizzle-orm/neon-http'
import { neon } from '@neondatabase/serverless'
import { sql } from 'drizzle-orm'

const sqlClient = neon(process.env.EMBEDDINGS_POSTGRES_URL!)
const db = drizzle(sqlClient)

// ‚úÖ Define TypeScript interface for DB results
interface EmbeddingResult {
  document_id: string
  collection_slug: string
  context: string // ‚úÖ Retrieves full text, not vectors
}

export async function POST(req: Request) {
  try {
    const { messages, userId } = await req.json()

    if (!messages || !Array.isArray(messages)) {
      return new Response(JSON.stringify({ error: 'Messages array is required.' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    console.log('üîπ Received Messages:', messages)
    const userPrompt: string = messages[messages.length - 1]?.content || 'Unknown query'

    // ‚úÖ Step 1: Generate Embedding for User Query
    console.log('üîç Generating embedding for user query...')
    const embeddingResponse = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: userPrompt,
    })

    const queryEmbedding: number[] = embeddingResponse?.data?.[0]?.embedding ?? []
    if (queryEmbedding.length !== 1536) {
      return new Response(JSON.stringify({ error: 'Invalid embedding generated by OpenAI.' }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    console.log('‚úÖ Embedding Generated:', queryEmbedding.slice(0, 5), '... (truncated)')

    // ‚úÖ Step 2: Query `embeddings` Table for Similar Matches
    console.log('üîç Querying the embeddings table for relevant context...')

    const query = sql`
      SELECT document_id, collection_slug, context
      FROM embeddings
      ORDER BY embedding <-> ${sql.raw(`'${JSON.stringify(queryEmbedding)}'::vector(1536)`)}
      LIMIT 3
    `

    const result = await db.execute(query)

    // ‚úÖ Extract relevant context from query results
    const rows: EmbeddingResult[] = result.rows.map((row) => ({
      document_id: row.document_id as string,
      collection_slug: row.collection_slug as string,
      context: row.context as string,
    }))

    console.log(`‚úÖ Found ${rows.length} relevant document(s)`)

    // ‚úÖ Step 3: Prepare Full Context for AI
    const contextData = rows.map(
      (row) => `Collection: ${row.collection_slug}\nContent: ${row.context}`,
    )
    const context: string = contextData.join('\n\n')

    console.log('üìÑ Retrieved Context for AI:', context)

    // ‚úÖ Step 4: Pass Chat History & Context to OpenAI
    const response = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [
        {
          role: 'system',
          content:
            "You are an AI assistant helping answer questions about Kyle Holloway's career and expertise. Use the provided context when available.",
        },
        { role: 'system', content: `Context:\n${context}` },
        ...messages,
      ],
      user: userId,
    })

    const aiResponse: string = response.choices[0]?.message?.content ?? 'No response received'

    console.log('‚úÖ OpenAI Response Received!')

    return new Response(JSON.stringify({ reply: aiResponse }), {
      status: 200,
      headers: { 'Content-Type': 'application/json' },
    })
  } catch (error) {
    console.error('‚ùå API Error:', error)
    return new Response(JSON.stringify({ error: 'An unexpected error occurred.' }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    })
  }
}
