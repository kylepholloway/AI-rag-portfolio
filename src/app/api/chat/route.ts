import { openai } from '@/utils/openai'
import { Pool } from 'pg'

const pool = new Pool({
  connectionString: process.env.POSTGRES_URL,
})

// ‚úÖ Define TypeScript interface for DB results
interface DBRow {
  title: string
  content: string
}

export async function POST(req: Request) {
  try {
    const { messages, userId } = await req.json()

    if (!messages || !Array.isArray(messages)) {
      return new Response(JSON.stringify({ error: 'Messages array is required.' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    console.log('üîπ Received Messages:', messages)
    const userPrompt: string = messages[messages.length - 1]?.content || 'Unknown query'

    // ‚úÖ Step 1: Generate Embedding for User Query
    const embeddingResponse = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: userPrompt,
    })

    const embedding: number[] = embeddingResponse?.data?.[0]?.embedding ?? []
    if (embedding.length !== 1536) {
      return new Response(JSON.stringify({ error: 'Invalid embedding generated by OpenAI.' }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    console.log('‚úÖ Embedding Generated:', embedding.slice(0, 5), '... (truncated)')

    // ‚úÖ Step 2: Query NeonDB for Relevant Content
    const collectionTables = [
      'work_experience',
      'projects',
      'skills',
      'articles',
      'qa',
      'fine_tuning_prompts',
    ]
    let contextData: string[] = []

    for (const table of collectionTables) {
      try {
        // ‚úÖ Ensure correct content column
        let contentColumn: string
        if (table === 'qa') contentColumn = 'answer'
        else if (table === 'fine_tuning_prompts') contentColumn = 'context'
        else contentColumn = 'content'

        // ‚úÖ Ensure correct title column
        let titleColumn: string = 'title'
        if (table === 'fine_tuning_prompts') titleColumn = 'prompt'

        const query = `
          SELECT ${titleColumn} AS title, ${contentColumn} AS content
          FROM ${table}
          ORDER BY (embedding->>'vector')::vector(1536) <-> $1::vector(1536)
          LIMIT 3
        `

        const { rows }: { rows: DBRow[] } = await pool.query(query, [embedding])

        rows.forEach((row: DBRow) => {
          contextData.push(`Title: ${row.title}\nContent: ${row.content}`)
        })

        console.log(`‚úÖ Retrieved ${rows.length} matches from ${table}`)
      } catch (error) {
        console.error(`‚ùå Query failed for table: ${table}`, error)
      }
    }

    const context: string = contextData.join('\n\n')

    console.log('üìÑ Retrieved Context for AI:', context)

    // ‚úÖ Step 3: Pass Chat History & Context to OpenAI
    const response = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [
        {
          role: 'system',
          content:
            "You are an AI assistant helping answer questions about Kyle Holloway's career and expertise. Use the provided context when available.",
        },
        { role: 'system', content: `Context:\n${context}` },
        ...messages,
      ],
      user: userId,
    })

    const aiResponse: string = response.choices[0]?.message?.content ?? 'No response received'

    console.log('‚úÖ OpenAI Response Received!')

    return new Response(JSON.stringify({ reply: aiResponse }), {
      status: 200,
      headers: { 'Content-Type': 'application/json' },
    })
  } catch (error) {
    console.error('‚ùå API Error:', error)
    return new Response(JSON.stringify({ error: 'An unexpected error occurred.' }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    })
  }
}
